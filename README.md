# Risk-Benchmarking-for-LLMs
# ğŸ” LLM Risk Benchmarking Toolkit

A practical framework to evaluate the **real-world vulnerabilities of open-source Large Language Models (LLMs)** â€” developed in collaboration with **Prediction Guard** and presented at **INFORMS 2025**.

---

## ğŸ§  Objective

LLMs are powerful â€” but just like people, theyâ€™re prone to mistakes under pressure. As these models enter healthcare, finance, and enterprise workflows, itâ€™s no longer enough to just ask *â€œHow accurate is it?â€*  

We need to ask:  
**â€œHow safe is it under real-world conditions?â€**  
This toolkit is our answer to that.

---

## ğŸ¯ Goals

- Evaluate model sensitivity to prompt variation (spelling errors, tone changes, format shifts)  
- Assess risks related to PII leakage, factual hallucinations, prompt injections, and toxic output  
- Create interpretable metrics like **Prompt Deviation Score (PDS)** and **Semantic Drift**  
- Benchmark the performance and safety tradeoffs between smaller and larger LLMs  
- Provide practical insights for **safer deployment** and **cost-effective model selection**

---

## ğŸ§ª Core Components

### 1. Prompt Robustness Testing
- Synonym swaps & rephrasings  
- Format and tone shifts (formal â†” casual)  
- Typographical noise (spelling errors)

### 2. Risk Categories Evaluated
- Prompt Sensitivity  
- Prompt Injection Vulnerability  
- Factual Consistency  
- Toxicity & Bias Output  
- PII Leakage (e.g., names, numbers)

### 3. Models Benchmarked
- `Hermes-2-Pro-LLaMA-3-8B`  
- `Hermes-3-LLaMA-3.1-70B`  
Tested using **SQuAD**, **Jigsaw**, and synthetic adversarial sets.

### 4. Custom Metrics Introduced
- **Prompt Deviation Score (PDS)** â€” how much model behavior changes with prompt tweaks  
- **Cosine Similarity Drift** â€” semantic shift measured via embeddings  
- **Toxicity Score** â€” using Googleâ€™s Perspective API  
- **Factual Entailment Rate** â€” judged via LLM-as-a-Judge methodology

---

## âš™ï¸ Tech Stack

- Python 3.10+  
- Hugging Face Transformers & Datasets  
- SentenceTransformers  
- Databricks / Colab (A100 GPUs recommended)  
- Google Perspective API (toxicity scoring)  
- MLflow (because repeatability is underrated)  
- Optional: OpenAI API for LLM-as-a-Judge evaluations

---

## ğŸ’¡ Key Insights

- **Smaller is smarter (sometimes):** The 8B model showed better factual reliability and resistance to prompt injection  
- **Semantic tweaks > typos:** Meaningful rewording caused more response drift than misspellings  
- **Efficiency wins:** Open-source models + GPU instances cut evaluation costs by over **90%** vs commercial APIs  
- **Enterprise strategy matters:** Safer models can be tiered for use cases â€” e.g., Hermes-2 for compliance-sensitive tasks, Hermes-3 for ideation

---

---

## ğŸ› ï¸ Getting Started

1. Open `SQuAD_Robustness.ipynb` or `Injection_Tests.ipynb`  
2. Choose your LLM(s) and evaluation criteria  
3. Run prompt tests â€” both original and adversarial  
4. Analyze PDS, similarity drift, and response risk scores  

> âš¡ **Bonus:** Easily runnable on Databricks or Colab â€” no heavy setup required.

---

## ğŸ”§ Applications

- âœ… AI Governance & Risk Management  
- âœ… Enterprise Model Selection & Audits  
- âœ… AI Trust & Safety Research  
- âœ… Cost-efficient evaluation pipelines for startups  
- âœ… Benchmarking open-source models for regulated domains

---

## ğŸ“‰ Limitations & Next Steps

- Current focus: English-only prompts and LLaMA-family models  
- Future expansion: Chat-based models (e.g., Mixtral), multilingual evaluation  
- Upcoming feature: Scenario-based red teaming prompts (e.g., healthcare triage, legal disclaimers)

> ğŸ’¡ Have suggestions or want to collaborate? Open an issue or reach out!

---

## ğŸ“Œ Acknowledgments

Huge thanks to **Prediction Guard** â€” especially *Aishwarya Ramasethu* â€” for mentorship, guidance, and helping us think like red-teamers.  
Thanks to the **open-source LLM community** (Nous, Hugging Face, Teknium), Googleâ€™s **Perspective API** team, and **Prof. Matthew Lanham** for his steady academic support.

---

## ğŸ“¬ Contact

ğŸ“§ avanti.c@purdue.edu  
ğŸ’¼ Project team: Abhishek Bagepalli, Ramya Polineni, Tsung-Yu Lu, Chan-Yen Hsiung, Jayesh Chaudhari, and Avanti Chandratre  
ğŸ“ Purdue University â€“ MSBAIM 2025

